{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Mvk02I578qBC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5 yolov5\n",
        "!pip install -qr yolov5/requirements.txt\n",
        "\n",
        "# Verify YOLOv5 installation\n",
        "!python --version\n",
        "!python -c \"import torch; print('PyTorch:', torch.__version__)\"\n",
        "!ls yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xF2wgtI3yIr",
        "outputId": "0d138f3a-91e3-4247-e07e-4b7cfd3990a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n",
            "Python 3.10.12\n",
            "PyTorch: 2.5.0+cu121\n",
            "benchmarks.py\t data\t     LICENSE\t     README.md\t       train.py        yolov5s.pt\n",
            "CITATION.cff\t detect.py   models\t     README.zh-CN.md   tutorial.ipynb\n",
            "classify\t export.py   __pycache__     requirements.txt  utils\n",
            "CONTRIBUTING.md  hubconf.py  pyproject.toml  segment\t       val.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_small_dataset(original_base_dir: Path, small_base_dir: Path, samples_per_split: Dict[str, int]):\n",
        "    \"\"\"\n",
        "    Create a smaller version of the dataset for testing.\n",
        "\n",
        "    Args:\n",
        "        original_base_dir: Path to original dataset\n",
        "        small_base_dir: Path to create small dataset\n",
        "        samples_per_split: Dict with number of samples for each split (train, val, test)\n",
        "    \"\"\"\n",
        "    print(\"Creating small dataset...\")\n",
        "\n",
        "    # Create directory structure\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for subdir in ['images', 'labels']:\n",
        "            os.makedirs(small_base_dir / split / subdir, exist_ok=True)\n",
        "\n",
        "    # Copy subset of files for each split\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        # Get list of all images\n",
        "        image_dir = original_base_dir / split / 'images'\n",
        "        all_images = list(image_dir.glob('*.jpg'))\n",
        "\n",
        "        # Randomly select subset\n",
        "        n_samples = min(samples_per_split[split], len(all_images))\n",
        "        selected_images = random.sample(all_images, n_samples)\n",
        "\n",
        "        print(f\"Copying {n_samples} files for {split} split...\")\n",
        "\n",
        "        # Copy selected files\n",
        "        for img_path in selected_images:\n",
        "            # Copy image\n",
        "            shutil.copy2(img_path, small_base_dir / split / 'images' / img_path.name)\n",
        "\n",
        "            # Copy corresponding label\n",
        "            label_path = original_base_dir / split / 'labels' / f\"{img_path.stem}.txt\"\n",
        "            if label_path.exists():\n",
        "                shutil.copy2(label_path, small_base_dir / split / 'labels' / f\"{img_path.stem}.txt\")\n",
        "\n",
        "    # Create data.yaml for small dataset\n",
        "    yaml_content = {\n",
        "        'train': str(small_base_dir / 'train' / 'images'),\n",
        "        'val': str(small_base_dir / 'val' / 'images'),\n",
        "        'test': str(small_base_dir / 'test' / 'images'),\n",
        "        'nc': 6,\n",
        "        'names': ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "    }\n",
        "\n",
        "    import yaml\n",
        "    with open(small_base_dir / 'data.yaml', 'w') as f:\n",
        "        yaml.dump(yaml_content, f, default_flow_style=False)\n",
        "\n",
        "    print(\"Small dataset created successfully!\")\n",
        "    return small_base_dir / 'data.yaml'"
      ],
      "metadata": {
        "id": "lI0lla-a9_ue"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_dataset(base_dir: Path):\n",
        "    \"\"\"Verify dataset structure and print statistics.\"\"\"\n",
        "    print(\"\\nVerifying dataset structure:\")\n",
        "\n",
        "    # Required directories\n",
        "    required_dirs = ['train/images', 'train/labels',\n",
        "                    'val/images', 'val/labels',\n",
        "                    'test/images', 'test/labels']\n",
        "\n",
        "    dataset_info = {}\n",
        "\n",
        "    for dir_path in required_dirs:\n",
        "        full_path = base_dir / dir_path\n",
        "        if not full_path.exists():\n",
        "            print(f\"Missing directory: {full_path}\")\n",
        "            return None\n",
        "\n",
        "        # Count files in directory\n",
        "        n_files = len(list(full_path.glob('*')))\n",
        "        print(f\"Found {dir_path}: {n_files} files\")\n",
        "        dataset_info[dir_path] = n_files\n",
        "\n",
        "    # Check for yaml file\n",
        "    yaml_path = base_dir / 'trashnet.yaml'\n",
        "    if yaml_path.exists():\n",
        "        print(f\"Found data.yaml at {yaml_path}\")\n",
        "    else:\n",
        "        print(f\"Missing data.yaml at {yaml_path}\")\n",
        "        return None\n",
        "\n",
        "    return dataset_info"
      ],
      "metadata": {
        "id": "ZKhtt_YQ34Qe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained YOLOv5 model"
      ],
      "metadata": {
        "id": "3JD0n-Pi89Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GarbageClassifier:\n",
        "    \"\"\"Garbage classification using YOLOv5.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize classifier with basic setup.\"\"\"\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load model\n",
        "        print(\"Loading YOLOv5 model...\")\n",
        "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "        self.model.to(self.device)\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "        # Setup directories\n",
        "        self.results_dir = os.path.abspath(\"training_results\")\n",
        "        os.makedirs(self.results_dir, exist_ok=True)\n",
        "\n",
        "        # Class names\n",
        "        self.class_names = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
        "\n",
        "    def train(self, data_yaml: str, epochs: int, batch_size: int = 16) -> str:\n",
        "        \"\"\"Train model and return results path.\"\"\"\n",
        "        if not os.path.exists(data_yaml):\n",
        "            raise FileNotFoundError(f\"data.yaml not found at {data_yaml}\")\n",
        "\n",
        "        exp_name = f\"exp_{epochs}_epochs\"\n",
        "        exp_path = os.path.join(self.results_dir, exp_name)\n",
        "\n",
        "        # Modified training command without --verbose\n",
        "        cmd = (f\"cd yolov5 && \"\n",
        "               f\"python train.py \"\n",
        "               f\"--img 640 \"\n",
        "               f\"--batch {batch_size} \"\n",
        "               f\"--epochs {epochs} \"\n",
        "               f\"--data {str(Path(data_yaml).absolute())} \"\n",
        "               f\"--weights yolov5s.pt \"\n",
        "               f\"--project {str(Path(self.results_dir).absolute())} \"\n",
        "               f\"--name {exp_name} \"\n",
        "               f\"--exist-ok \"  # Add this to overwrite existing experiments\n",
        "               f\"--patience 0\")  # Disable early stopping\n",
        "\n",
        "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "        print(f\"Training command: {cmd}\")\n",
        "\n",
        "        try:\n",
        "            process = subprocess.run(\n",
        "                cmd,\n",
        "                shell=True,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=True\n",
        "            )\n",
        "            print(\"Training output:\")\n",
        "            print(process.stdout)\n",
        "            if process.stderr:\n",
        "                # Filter out wandb warnings\n",
        "                stderr_lines = [line for line in process.stderr.split('\\n')\n",
        "                              if 'wandb' not in line.lower() and line.strip()]\n",
        "                if stderr_lines:\n",
        "                    print(\"Errors:\", '\\n'.join(stderr_lines))\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Training failed with error: {e}\")\n",
        "            if e.output:\n",
        "                print(\"Output:\", e.output)\n",
        "            if e.stderr:\n",
        "                print(\"Error output:\", e.stderr)\n",
        "            return None\n",
        "\n",
        "        # Check for results file\n",
        "        results_file = os.path.join(exp_path, 'results.csv')\n",
        "        if not os.path.exists(results_file):\n",
        "            print(f\"Warning: No results file found at {results_file}\")\n",
        "            return None\n",
        "\n",
        "        return exp_path"
      ],
      "metadata": {
        "id": "uf5pcrpS865y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_and_verify_yaml(base_dir: str) -> str:\n",
        "#     \"\"\"Create and verify data.yaml file.\"\"\"\n",
        "#     # Convert paths to absolute and handle spaces correctly\n",
        "#     train_path = str(Path(base_dir) / 'train' / 'images')\n",
        "#     val_path = str(Path(base_dir) / 'val' / 'images')\n",
        "#     test_path = str(Path(base_dir) / 'test' / 'images')\n",
        "\n",
        "#     # Verify directories exist\n",
        "#     for path in [train_path, val_path, test_path]:\n",
        "#         if not os.path.exists(path):\n",
        "#             raise FileNotFoundError(f\"Directory not found: {path}\")\n",
        "\n",
        "#     data_config = {\n",
        "#         'train': train_path,\n",
        "#         'val': val_path,\n",
        "#         'test': test_path,\n",
        "#         'nc': 6,\n",
        "#         'names': ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "#     }\n",
        "\n",
        "#     yaml_path = str(Path(base_dir) / 'trashnet.yaml')\n",
        "\n",
        "#     # Save YAML file\n",
        "#     import yaml\n",
        "#     with open(yaml_path, 'w') as f:\n",
        "#         yaml.dump(data_config, f, default_flow_style=False)\n",
        "\n",
        "#     # Verify YAML file was created and is readable\n",
        "#     with open(yaml_path, 'r') as f:\n",
        "#         loaded_config = yaml.safe_load(f)\n",
        "#         print(\"\\nCreated data.yaml with contents:\")\n",
        "#         print(yaml.dump(loaded_config, default_flow_style=False))\n",
        "\n",
        "#     return yaml_path"
      ],
      "metadata": {
        "id": "Z9Hz-aNlypeH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_plot(classifier: GarbageClassifier, data_yaml: str, epochs_list: List[int]):\n",
        "    \"\"\"Train models and collect results.\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for epochs in epochs_list:\n",
        "        exp_path = classifier.train(data_yaml, epochs)\n",
        "        if exp_path is None:\n",
        "            print(f\"Skipping results for {epochs} epochs due to training failure\")\n",
        "            continue\n",
        "\n",
        "        results_file = os.path.join(exp_path, 'results.csv')\n",
        "        try:\n",
        "            df = pd.read_csv(results_file)\n",
        "            results[epochs] = {\n",
        "                'train_loss': df['train/box_loss'].values,\n",
        "                'val_loss': df['val/box_loss'].values,\n",
        "                'mAP50': df['metrics/mAP_0.5'].values  # Keep all values for plotting\n",
        "            }\n",
        "            print(f\"\\nTraining completed for {epochs} epochs\")\n",
        "            print(f\"Final mAP50: {results[epochs]['mAP50'][-1]:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing results for {epochs} epochs: {str(e)}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "lwtzg_OaEiz4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(results: Dict):\n",
        "    \"\"\"Plot training vs validation metrics.\"\"\"\n",
        "    if not results:\n",
        "        print(\"No results to plot\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "\n",
        "        # Get the epochs with the most data points (usually the longest training run)\n",
        "        best_epoch = max(results.keys(), key=lambda k: len(results[k]['train_loss']))\n",
        "        epochs_range = range(1, len(results[best_epoch]['train_loss']) + 1)\n",
        "\n",
        "        # Plot training and validation loss\n",
        "        plt.plot(epochs_range, results[best_epoch]['train_loss'],\n",
        "                label='Training Loss', color='blue')\n",
        "        plt.plot(epochs_range, results[best_epoch]['val_loss'],\n",
        "                label='Validation Loss', color='red')\n",
        "\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training and Validation Loss Over Time')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting results: {str(e)}\")"
      ],
      "metadata": {
        "id": "3GsKZuoQnCdv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Original dataset path\n",
        "        original_base_dir = Path(\"/content/drive/MyDrive/ColabNotebooks_New/data/TrashNet\")\n",
        "\n",
        "        # Create small dataset\n",
        "        small_base_dir = Path(\"/content/small_trashnet\")\n",
        "        samples_per_split = {\n",
        "            'train': 100,  # 100 images for training\n",
        "            'val': 50,     # 50 images for validation\n",
        "            'test': 20     # 20 images for testing\n",
        "        }\n",
        "\n",
        "        # Create small dataset and get yaml path\n",
        "        #data_yaml = create_small_dataset(original_base_dir, small_base_dir, samples_per_split)\n",
        "\n",
        "        data_yaml = \"/content/small_trashnet/data.yaml\"\n",
        "\n",
        "        # Initialize classifier\n",
        "        classifier = GarbageClassifier()\n",
        "\n",
        "        # Train with a single epoch\n",
        "        epochs_list = [1]\n",
        "        results = train_and_plot(classifier, str(data_yaml), epochs_list)\n",
        "\n",
        "        # Plot results\n",
        "        plot_results(results)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "_Gcrkg9gnDa2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRXB3ULWp47X",
        "outputId": "7d71b13e-af09-482b-d6b2-a9bc7f31be0a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeuun06knPfb",
        "outputId": "374ec68f-fe88-4a52-a147-c217729c698f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loading YOLOv5 model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2024-11-12 Python-3.10.12 torch-2.5.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n",
            "\n",
            "Starting training for 1 epochs...\n",
            "Training command: cd yolov5 && python train.py --img 640 --batch 16 --epochs 1 --data /content/small_trashnet/data.yaml --weights yolov5s.pt --project /content/training_results --name exp_1_epochs --exist-ok --patience 0\n",
            "Training output:\n",
            "\n",
            "Errors: 2024-11-12 21:27:37.852495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-12 21:27:37.888062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-12 21:27:37.898510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/small_trashnet/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/content/training_results, name=exp_1_epochs, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=0, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-383-g1435a8ee Python-3.10.12 torch-2.5.0+cu121 CPU\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/training_results', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     29667  models.yolo.Detect                      [6, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7035811 parameters, 7035811 gradients, 16.0 GFLOPs\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/small_trashnet/train/labels...:   0%|          | 0/125 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/small_trashnet/train/labels... 106 images, 2 backgrounds, 0 corrupt:  86%|████████▌ | 107/125 [00:00<00:00, 1069.48it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/small_trashnet/train/labels... 124 images, 2 backgrounds, 0 corrupt: 100%|██████████| 125/125 [00:00<00:00, 1055.58it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/small_trashnet/train/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/small_trashnet/val/labels.cache... 46 images, 4 backgrounds, 0 corrupt: 100%|██████████| 50/50 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/small_trashnet/val/labels.cache... 46 images, 4 backgrounds, 0 corrupt: 100%|██████████| 50/50 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m1.35 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to /content/training_results/exp_1_epochs/labels.jpg... \n",
            "/content/yolov5/train.py:355: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/training_results/exp_1_epochs\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "  0%|          | 0/8 [00:00<?, ?it/s]/content/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/0         0G    0.07381     0.0301    0.04171         50        640:   0%|          | 0/8 [00:52<?, ?it/s]\n",
            "        0/0         0G    0.07381     0.0301    0.04171         50        640:  12%|█▎        | 1/8 [00:56<06:36, 56.61s/it]/content/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/0         0G    0.07225    0.03017    0.03891         48        640:  12%|█▎        | 1/8 [01:33<06:36, 56.61s/it]\n",
            "        0/0         0G    0.07225    0.03017    0.03891         48        640:  25%|██▌       | 2/8 [01:33<04:30, 45.07s/it]/content/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/0         0G    0.07319    0.03051    0.03914         51        640:  25%|██▌       | 2/8 [02:03<04:30, 45.07s/it]\n",
            "        0/0         0G    0.07319    0.03051    0.03914         51        640:  38%|███▊      | 3/8 [02:03<03:10, 38.04s/it]/content/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/0         0G    0.07216    0.03035    0.03858         46        640:  38%|███▊      | 3/8 [02:32<03:10, 38.04s/it]\n",
            "        0/0         0G    0.07216    0.03035    0.03858         46        640:  50%|█████     | 4/8 [02:32<02:18, 34.58s/it]/content/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/0         0G    0.07196    0.03008    0.03811         47        640:  50%|█████     | 4/8 [03:02<02:18, 34.58s/it]\n",
            "        0/0         0G    0.07196    0.03008    0.03811         47        640:  62%|██████▎   | 5/8 [03:02<01:38, 32.87s/it]/content/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/0         0G    0.07212    0.02983    0.03841         48        640:  62%|██████▎   | 5/8 [03:30<01:38, 32.87s/it]\n",
            "        0/0         0G    0.07212    0.02983    0.03841         48        640:  75%|███████▌  | 6/8 [03:30<01:02, 31.09s/it]/content/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/0         0G    0.07057    0.02984    0.03849         50        640:  75%|███████▌  | 6/8 [03:56<01:02, 31.09s/it]\n",
            "        0/0         0G    0.07057    0.02984    0.03849         50        640:  88%|████████▊ | 7/8 [03:56<00:29, 29.67s/it]/content/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/0         0G    0.06853    0.03006    0.03816         38        640:  88%|████████▊ | 7/8 [04:17<00:29, 29.67s/it]\n",
            "        0/0         0G    0.06853    0.03006    0.03816         38        640: 100%|██████████| 8/8 [04:17<00:00, 26.95s/it]\n",
            "        0/0         0G    0.06853    0.03006    0.03816         38        640: 100%|██████████| 8/8 [04:17<00:00, 32.24s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:17<00:17, 17.76s/it]WARNING ⚠️ NMS time limit 1.400s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:26<00:00, 12.51s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:26<00:00, 13.30s/it]\n",
            "                   all         50         46    0.00303      0.497     0.0263     0.0208\n",
            "1 epochs completed in 0.079 hours.\n",
            "Optimizer stripped from /content/training_results/exp_1_epochs/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from /content/training_results/exp_1_epochs/weights/best.pt, 14.4MB\n",
            "Validating /content/training_results/exp_1_epochs/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7026307 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:14<00:14, 14.88s/it]WARNING ⚠️ NMS time limit 1.400s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:26<00:00, 13.15s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:26<00:00, 13.41s/it]\n",
            "                   all         50         46    0.00301      0.453     0.0222     0.0159\n",
            "             cardboard         50          6    0.00079      0.167    0.00186     0.0013\n",
            "                 glass         50          8    0.00273       0.75     0.0585     0.0277\n",
            "                 metal         50          6    0.00282      0.667     0.0598     0.0574\n",
            "                 paper         50         16    0.00893      0.375    0.00676    0.00291\n",
            "               plastic         50          7    0.00225      0.429      0.006      0.006\n",
            "                 trash         50          3   0.000551      0.333   0.000414   8.27e-05\n",
            "Results saved to \u001b[1m/content/training_results/exp_1_epochs\u001b[0m\n",
            "Error processing results for 1 epochs: 'train/box_loss'\n",
            "No results to plot\n"
          ]
        }
      ]
    }
  ]
}